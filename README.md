# AI Model and Backend for prototype project better park

## Setup

```bash
pip install -r requirements.txt
```

YOLOv8 weights (`yolov8n.pt`) will download automatically on first run (~6MB).

---

## Quick Start

### Lite tier - just count cars and get occupancy %

```bash
python run.py --video your_parking_lot.mp4 --capacity 50
```

This shows a live window with bounding boxes around detected cars and a HUD showing:
- Vehicles detected
- Available spots
- Occupancy %

### Pro tier - spot-level detection

First, run the calibration tool on your video to define each spot:
```bash
python calibrate.py --video your_parking_lot.mp4 --output spots.json
```

Draw a polygon around each empty parking spot (left click to add points, Enter to save a spot).

Then run with the spots file:
```bash
python run.py --video your_parking_lot.mp4 --capacity 50 --spots spots.json
```

Each spot will show green (open) or red (occupied).

---

## Options

| Flag         | Default       | Description                              |
|--------------|---------------|------------------------------------------|
| `--video`    | required      | Path to input video                      |
| `--capacity` | 100           | Total number of spots in the lot         |
| `--model`    | yolov8n.pt    | YOLO model (n=fast, s=balanced, m=accurate) |
| `--conf`     | 0.4           | Confidence threshold (0.0 - 1.0)         |
| `--spots`    | None          | Path to spots.json for pro tier          |
| `--save`     | None          | Save annotated video to this path        |
| `--skip`     | 2             | Process every Nth frame (higher = faster)|

---

## File Structure

```
parking_cv/
├── detector.py       # Core ParkingDetector class
├── run.py            # Video runner script
├── calibrate.py      # Spot calibration tool (pro tier)
├── requirements.txt
└── spots.json        # Generated by calibrate.py (pro tier)
```

---

## Next Steps

Once this pipeline is working, the next layer is:
1. Finetune model with parking garage datesets
2. A **FastAPI backend** that runs the detector and exposes occupancy data via REST/WebSocket
3. A **React frontend** with the map UI showing green/red spots or occupancy % per lot
